## 1.6続き p.51

- 分布が偏っているとエントロピーが低い、満遍なく分布しているとエントロピーが高い
- エントロピー最大の確率分布の求め方
  - 方法1
    - ほぼ `H` を最大化
    - ただし、「 `H` には `p` が規格化されている条件」をラグランジュ乗数法使って組み込む → 式(1.99)
    - 解き方については付録E「ラグランジュ乗数法」を参考に
    - 式(1.99)が最大となるのは確率 `p` が全て等しい時 → `H = ln M`
  - 方法2
    - イェンセンの不等式 演習(1.29)
- 停留点が最大値であることを確認する
  - エントロピーの2回微分 式(1.100)
  - 2回微分は傾きの変化率、これが負の値だから上に凸 (訳註)
    - 訳註には凹関数って書いてるけど…あれ？(´・ω・｀)
- 離散値について考えてきたけど、連続値に拡張してみる → 微分エントロピー
  - 確率 `p` が連続であると仮定する
    - 平均値の定理を利用できる
  - `i` に対して式(1.101)が成り立つような `i * delta <= x_i <= (i + 1) * delta ` が必ず存在する
  - `i * delta <= x <= (i + 1) * delta ` となる `x` を`x_i` に割り当てる(量子化する)
  - 少子化すると式(1.102)が得られる
  - よくある極限に飛ばすやつで式(1.103)を得る
- 離散と連続の違い
  - `- ln(delta)` が違う
  - 厳密に連続変数を扱うには無限ビット必要であることを意味する
  - 密度…？
- 連続変数の場合のエントロピー最大化
  - 離散変数なら等確率の場合
  - 規格化 式(1.105) に加え、1・2次モーメント 式(1.106) 式(1.107) を考慮する必要がある
    - 平均や分散が無限に発散しないことを制限するため…？？
  - ラグランジュ乗数法や変分法を使って計算すると 式(1.109) のように確率 `p` がもとまる
    - が、ガウス分布やん…！！！！！（わざとらしい
    - 分布が非負である制約は置かなかったけど、結果として非負になったね
  - 求めた `p` を使ってエントロピーを計算すると…
    - 分散が大きいとエントロピーも大きくなる
    - 連続変数の場合はエントロピーが負になることもある
- 条件付きエントロピー 式(1.111)
  - x かつ y となる確率は `p(y, x)`
  - 驚きの度合いとしては、すでに `x` であることを知っているので `-ln(p(y|x))` となる
    - `p(y|x) > p(x,y)` なので `-ln(p(y|x)) > -ln(p(x,y))` 驚きが減る

## 1.6.1 相対エントロピーと相互情報量

- 情報理論とパターン認識を結びつける
- 相対エントロピー/カルバック・ライブラーダイバージェンス/KLダイバージェンス 式(1.113)
  - 非対称であることに注意
  - 非負
  - 等式が成り立つ <=> `p(x) = q(y)`
- 凸関数の概念
  - 式(1.114)が成り立つことと、2階微分が正であることは等価
