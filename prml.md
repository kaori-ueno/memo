## 1.6続き p.51

- 分布が偏っているとエントロピーが低い、満遍なく分布しているとエントロピーが高い
- エントロピー最大の確率分布の求め方
  - 方法1
    - ほぼ `H` を最大化
    - ただし、「 `H` には `p` が規格化されている条件」をラグランジュ乗数法使って組み込む → 式(1.99)
    - 解き方については付録E「ラグランジュ乗数法」を参考に
    - 式(1.99)が最大となるのは確率 `p` が全て等しい時 → `H = ln M`
  - 方法2
    - イェンセンの不等式 演習(1.29)
- 停留点が最大値であることを確認する
  - エントロピーの2回微分 式(1.100)
  - 2回微分は傾きの変化率、これが負の値だから上に凸 (訳註)
    - 訳註には凹関数って書いてるけど…あれ？(´・ω・｀)
- 離散値について考えてきたけど、連続値に拡張してみる → 微分エントロピー
  - 確率 `p` が連続であると仮定する
    - 平均値の定理を利用できる
  - `i` に対して式(1.101)が成り立つような `i * delta <= x_i <= (i + 1) * delta ` が必ず存在する
  - `i * delta <= x <= (i + 1) * delta ` となる `x` を`x_i` に割り当てる(量子化する)
  - 少子化すると式(1.102)が得られる
  - よくある極限に飛ばすやつで式(1.103)を得る
- 離散と連続の違い
  - `- ln(delta)` が違う
  - 厳密に連続変数を扱うには無限ビット必要であることを意味する
  - 密度…？
- 連続変数の場合のエントロピー最大化
  - 離散変数なら等確率の場合
  - 規格化 式(1.105) に加え、1・2次モーメント 式(1.106) 式(1.107) を考慮する必要がある
    - 平均や分散が無限に発散しないことを制限するため…？？
  - ラグランジュ乗数法や変分法を使って計算すると 式(1.109) のように確率 `p` がもとまる
    - が、ガウス分布やん…！！！！！（わざとらしい
    - 分布が非負である制約は置かなかったけど、結果として非負になったね
  - 求めた `p` を使ってエントロピーを計算すると…
    - 分散が大きいとエントロピーも大きくなる
    - 連続変数の場合はエントロピーが負になることもある
- 条件付きエントロピー 式(1.111)
  - x かつ y となる確率は `p(y, x)`
  - 驚きの度合いとしては、すでに `x` であることを知っているので `-ln(p(y|x))` となる
    - `p(y|x) > p(x,y)` なので `-ln(p(y|x)) > -ln(p(x,y))` 驚きが減る

## 1.6.1 相対エントロピーと相互情報量

- 情報理論とパターン認識を結びつける
- 相対エントロピー/カルバック・ライブラーダイバージェンス/KLダイバージェンス 式(1.113)
  - 非対称であることに注意
  - 非負
  - 等式が成り立つ <=> `p(x) = q(y)`
- 凸関数の概念
  - 式(1.114)が成り立つことと、2階微分が正であることは等価

# 1章復習

- 学習の流れ
  - モデルを決める
      - 例：多項式曲線 式(1.1)
        - `M` はモデルの複雑さ
  - パラメータを決める
    - 誤差関数
      - 例：二乗和誤差 式(1.2)
        - 式の係数 `1/2` は微分した時に綺麗に収まるように
    - 誤差関数の最小化によりパラメータが決定できる => 最適化問題 演習(1.1)
- モデルの複雑さはどう決める？ 図(1.3)
  - データの数に対してモデルを複雑にしすぎると **過学習** してしまう
  - 過学習しないためには？
    - データを増やす
    - データを2つにわけて学習する
      - パラメータ決定用のデータ
      - モデルの複雑さが適切であるかを判断する用のデータ => **般化性能**
    - ベイズアプローチを用いる [1.3.4]
    - 正則化
      - 誤差関数に罰則項を加える 式(1.4)
      - `lamda` は罰則項の重み (適切に選ぶこと！！！)
- [1.2] 確率論
  - 加法定理・乗法定理・ベイズの定理
  - 事前分布
  - 事後分布
- [1.2.5] ベイズで曲線フィッティング


## 二項分布

### 2.1.1 ベータ分布

- 共役性: 事前分布と事後分布の形が同じ関数形式
  - ベータ分布は共役性をもつ

## 2.2 多値変数

- K種類の状態を扱う、K次元ベクトルで表す
- K=2の場合がベルヌーイ分布

### 2.2.1 ディリクレ分布

- 二項分布の事前分布といえばベータ分布、多項分布の事前分布といえばディレクリ分布
- 単体(simplec): ミューの定義域的なもの？

## 2.3 ガウス分布

- 1次元の話は1章でやったので、今回は多値変数に対するガウス分布のお話！
- マハラノビス距離 式(2.44)
  - Σが単位行列 → ユークリッド距離

### 2.3.4 ガウス分布の最尤推定

- 式(1.119) の二式は、 `x_i * x_j` のこと
- 式(2.122) は 式(1.56)の多次元版

### 2.3.5 逐次推定

- ガウス分布での逐次推定 式(2.126)
- 汎用的な方法は Robbins-Monro で求められるよ！

### 2.3.9 混合ガウス分布

- 混合分布とは！ 図(2.21)
  - 複数の分布を合わせるとより柔軟なモデルが得られる
- 混合ガウス分布 式(2.188)
  - 十分な数のガウス分布、任意の連続な密度関数を任意の精度で近似できる
  - 混合係数の条件 式(2.189) 式(2.190)
  - 周辺密度は 式(2.191)
    - `k` 番目のガウス分布を選ぶ確率 `p(k)`
  - 事後確率 `p(k|x)` は **負担率** と呼ばれる
  - 対数尤度関数 式(2.193)
    - 今までの式より複雑なので足し算に分解できない！！
    - 9章で出てくる **EMアルゴリズム** でできるのん

## 2.4 指数型分布族

- これまで出てきた分布は **指数型分布族** である（ただし混合ガウス分布をのぞく
- 定義 式(2.194)
  - 式中の `x` は離散値でも連続値でもよい
- ベルヌーイ分布が指数型分布であることを確かめる！
  - 無理矢理 `exp` の肩に乗っけてからのごにょごにょ 式(2.197)
  - `μ` は `η` の**ロジスティックシグモイド関数** になる 式(2.199)
    - `0 < μ < 1`
    - `μ` `η` についての式が求めておく 式(2.198)
    - いろんなところでででくるお
      - 確率っぽいものに変換したい！
      - ニューラルネットワーク
- 多項分布で確かめる！
  - `μ` は `η` の **ソフトマックス関数** or **正規化指数関数** になる 式(2.123)
    - `0 < μ < 1`
- ちなみに指数型分布族でない分布は？
  - 階二乗分布
  - T分布？ P分布？

## 2.5 ノンパラメトリック法

- 今まではあるモデルに対し、適切なパラメータを推定する問題だった
- パラメータを利用しない方法を考えていく
- ヒストグラム密度推定法
  - 適切な区切り幅を見出す必要がある
  - 欠点
    - 不連続なモデル
      - 積分できない
    - 次元の呪い
      - データの次元が大きくなると計算量やばい
      - 区間も爆増するのでデータが大量に必要になる
  - わかること
    - 確率密度推定のとき、近傍のデータも考慮する必要がある
    - 平滑化パラメータを適切に決める
- カーネル密度推定法
  - 計算！
    - あるデータ点 `x` を含む領域 `R` の確率 `P` は、式(2.242)
  - カーネル関数を使い分ければ色々できる
    - Parzen窓の場合 式(2.249)
      - 結局階段状になっちゃう・・・
    - ガウス関数の場合 式(2.249)
      - ちょっと滑らかになった！
  - 平滑化パラメータは h

### 2.5.2 再近傍法

- 近傍法では、K個の入る区間をそれぞれに考える
  - 密度推定法では決まった区間に入る店の数を考えた
- `k = 1` の場合、最近傍法
- `k > 1` の場合、K-NN

# 2章おさらい

- 二値
  - ベルヌーイ分布
  - 二項分布
  - ベータ分布
    - ベルヌーイ分布、二項分布の事前分布として有用
- 多値
  - 多項分布
  - ディレクリ分布
    - 多項分布の事前分布として有用
- ノンパラメトリック
  - パラメトリックより柔軟だが、
